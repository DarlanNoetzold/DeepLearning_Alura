{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Carregamento de Dados II.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDM84H0wO5I_",
        "colab_type": "text"
      },
      "source": [
        "# Carregamento de Dados\n",
        "\n",
        "Objetivos dessa aula:\n",
        "* Carregar um dataset customizado\n",
        "* Implementar o fluxo de treinamento **e validação** completo de uma rede\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg_W2-tDlpz3",
        "colab_type": "text"
      },
      "source": [
        "## Hiperparâmetros\n",
        "\n",
        "Vamos manter a organização do último script :)\n",
        "\n",
        "* imports de pacotes\n",
        "* configuração de hiperparâmetros\n",
        "* definição do hardware padrão utilizado\n",
        "\n",
        "E bora de GPU de novo! \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfisIdsWlioB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "70b1ec97-38f8-4123-b694-cad9352a80fb"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Configurando hiperparâmetros.\n",
        "args = {\n",
        "    'epoch_num': 200,     # Número de épocas.\n",
        "    'lr': 5e-5,           # Taxa de aprendizado.\n",
        "    'weight_decay': 5e-4, # Penalidade L2 (Regularização).\n",
        "    'num_workers': 3,     # Número de threads do dataloader.\n",
        "    'batch_size': 20,     # Tamanho do batch.\n",
        "}\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    args['device'] = torch.device('cuda')\n",
        "else:\n",
        "    args['device'] = torch.device('cpu')\n",
        "\n",
        "print(args['device'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyXDulgH77_s",
        "colab_type": "text"
      },
      "source": [
        "## Dataset \n",
        "\n",
        "Dataset de aplicativos para aluguel de bicicletas (*Bike Sharing Dataset*). <br>\n",
        "* Dadas algumas informações como velocidade do vento, estação do ano, etc., quantas bicicletas serão alugadas na próxima hora?\n",
        "\n",
        "Esse é um problema de **Regressão**, onde precisamos estimar uma variável dependente em um espaço contínuo (alugueis de bikes) a partir de um conjunto de variáveis independentes (as condições no momento)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ydTqRvqH-1-",
        "colab_type": "text"
      },
      "source": [
        "### Baixando o dataset\n",
        "\n",
        "Fonte: https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecgKLh2aKYH5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "d2e9f410-2796-4127-c845-42315aa66c72"
      },
      "source": [
        "! wget https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip\n",
        "! unzip Bike-Sharing-Dataset.zip  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-17 18:03:16--  https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 279992 (273K) [application/x-httpd-php]\n",
            "Saving to: ‘Bike-Sharing-Dataset.zip’\n",
            "\n",
            "Bike-Sharing-Datase 100%[===================>] 273.43K   511KB/s    in 0.5s    \n",
            "\n",
            "2019-10-17 18:03:17 (511 KB/s) - ‘Bike-Sharing-Dataset.zip’ saved [279992/279992]\n",
            "\n",
            "Archive:  Bike-Sharing-Dataset.zip\n",
            "  inflating: Readme.txt              \n",
            "  inflating: day.csv                 \n",
            "  inflating: hour.csv                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjsCxBQJsTce",
        "colab_type": "text"
      },
      "source": [
        "### Visualizando os dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUkvnM8SKlY3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "96916e77-bf6d-4dfa-eac8-524495c4c5e1"
      },
      "source": [
        "df = pd.read_csv('hour.csv')\n",
        "print(len(df))\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17379\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>instant</th>\n",
              "      <th>dteday</th>\n",
              "      <th>season</th>\n",
              "      <th>yr</th>\n",
              "      <th>mnth</th>\n",
              "      <th>hr</th>\n",
              "      <th>holiday</th>\n",
              "      <th>weekday</th>\n",
              "      <th>workingday</th>\n",
              "      <th>weathersit</th>\n",
              "      <th>temp</th>\n",
              "      <th>atemp</th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>casual</th>\n",
              "      <th>registered</th>\n",
              "      <th>cnt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>13</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8</td>\n",
              "      <td>32</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.2727</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5</td>\n",
              "      <td>27</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>2011-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2879</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   instant      dteday  season  yr  ...  windspeed  casual  registered  cnt\n",
              "0        1  2011-01-01       1   0  ...        0.0       3          13   16\n",
              "1        2  2011-01-01       1   0  ...        0.0       8          32   40\n",
              "2        3  2011-01-01       1   0  ...        0.0       5          27   32\n",
              "3        4  2011-01-01       1   0  ...        0.0       3          10   13\n",
              "4        5  2011-01-01       1   0  ...        0.0       0           1    1\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcvrTUl2OLmL",
        "colab_type": "text"
      },
      "source": [
        "### Tratamento de dados\n",
        "\n",
        "**Variáveis Categóricas** <br>\n",
        "Como descrito na página do dataset, apenas as variáveis numéricas estão normalizadas. No caso das categóricas (como dia da semana e estação do ano), cada elemento contém o índice da categoria.\n",
        "\n",
        "Existem várias formas de lidar com variáveis categóricas em uma regressão, mas para não desviar o foco da nossa aula manteremos os valores originais das variáveis categóricas.\n",
        "\n",
        "**Separação em treino e teste**<br>\n",
        "\n",
        "Para treinar e validar o nosso modelo, precisamos de dois conjuntos de dados (treino e teste). Para isso, utilizaremos a função ```torch.randperm``` para amostrar aleatoriamente um percentual dos dados, separando-os para validação.\n",
        "\n",
        "Documentação: https://pytorch.org/docs/stable/torch.html#torch.randperm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnjCdm1bMqhG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "7c307c94-e1bd-433c-955d-712da1f20feb"
      },
      "source": [
        "# Train/Test split\n",
        "torch.manual_seed(1)\n",
        "indices = torch.randperm(len(df)).tolist()\n",
        "\n",
        "train_size = int(0.8*len(df))\n",
        "df_train = df.iloc[indices[:train_size]]\n",
        "df_test  = df.iloc[indices[train_size:]]\n",
        "\n",
        "print(len(df_train), len(df_test))\n",
        "display(df_test.head())\n",
        "\n",
        "df_train.to_csv('bike_train.csv',index=False)\n",
        "df_test.to_csv('bike_test.csv',index=False)\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13903 3476\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>instant</th>\n",
              "      <th>dteday</th>\n",
              "      <th>season</th>\n",
              "      <th>yr</th>\n",
              "      <th>mnth</th>\n",
              "      <th>hr</th>\n",
              "      <th>holiday</th>\n",
              "      <th>weekday</th>\n",
              "      <th>workingday</th>\n",
              "      <th>weathersit</th>\n",
              "      <th>temp</th>\n",
              "      <th>atemp</th>\n",
              "      <th>hum</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>casual</th>\n",
              "      <th>registered</th>\n",
              "      <th>cnt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12663</th>\n",
              "      <td>12664</td>\n",
              "      <td>2012-06-16</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.6212</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.1940</td>\n",
              "      <td>123</td>\n",
              "      <td>229</td>\n",
              "      <td>352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1801</th>\n",
              "      <td>1802</td>\n",
              "      <td>2011-03-20</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.3939</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.3582</td>\n",
              "      <td>58</td>\n",
              "      <td>98</td>\n",
              "      <td>156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16567</th>\n",
              "      <td>16568</td>\n",
              "      <td>2012-11-28</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.2576</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.2239</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8817</th>\n",
              "      <td>8818</td>\n",
              "      <td>2012-01-08</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.3333</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.1045</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2608</th>\n",
              "      <td>2609</td>\n",
              "      <td>2011-04-23</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.5455</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.3582</td>\n",
              "      <td>182</td>\n",
              "      <td>209</td>\n",
              "      <td>391</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       instant      dteday  season  yr  ...  windspeed  casual  registered  cnt\n",
              "12663    12664  2012-06-16       2   1  ...     0.1940     123         229  352\n",
              "1801      1802  2011-03-20       1   0  ...     0.3582      58          98  156\n",
              "16567    16568  2012-11-28       4   1  ...     0.2239       0          12   12\n",
              "8817      8818  2012-01-08       1   1  ...     0.1045       0           2    2\n",
              "2608      2609  2011-04-23       2   0  ...     0.3582     182         209  391\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Bike-Sharing-Dataset.zip  bike_train.csv  hour.csv    sample_data\n",
            "bike_test.csv\t\t  day.csv\t  Readme.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daUL9nMxICW0",
        "colab_type": "text"
      },
      "source": [
        "### Classe Dataset\n",
        "\n",
        "O pacote ```torch.util.data``` possui a classe abstrata ```Dataset```. Ela permite que você implemente o seu próprio dataset reescrevendo os métodos:\n",
        "\n",
        "* ```__init__(self)```: Define a lista de amostras do seu dataset\n",
        "* ```__getitem__(self, idx)```: Carrega uma amostra, aplica as devidas transformações e retorna uma **tupla ```(dado, rótulo)```**.\n",
        "* ```__len__(self)```: Retorna a quantidade de amostras do dataset\n",
        "\n",
        "Tutorial completo do PyTorch: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaLPZteS9fkW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Bicicletinha(Dataset):\n",
        "  def __init__(self, csv_path, scaler_feat=None, scaler_label=None):\n",
        "  \n",
        "    self.dados = pd.read_csv(csv_path).to_numpy()\n",
        "    \n",
        "  def __getitem__(self, idx):\n",
        "    \n",
        "    sample = self.dados[idx][2:14]\n",
        "    label  = self.dados[idx][-1:]\n",
        "    \n",
        "    # converte para tensor\n",
        "    sample = torch.from_numpy(sample.astype(np.float32))\n",
        "    label  = torch.from_numpy(label.astype(np.float32))\n",
        "    \n",
        "    return sample, label\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.dados)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sd4yl7sbQQGr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "eaf1f6e9-af4c-4302-e780-a453d99c4411"
      },
      "source": [
        "dataset = Bicicletinha('bike_train.csv')\n",
        "dado, rotulo = dataset[0]\n",
        "print(rotulo)\n",
        "print(dado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([373.])\n",
            "tensor([ 4.0000,  1.0000, 11.0000, 19.0000,  0.0000,  4.0000,  1.0000,  1.0000,\n",
            "         0.3800,  0.3939,  0.2700,  0.3582])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwS8q0l0v8cP",
        "colab_type": "text"
      },
      "source": [
        "### Construindo conjuntos de treino e teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G8nAUzhxUlrZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "344cf7b1-8650-42d0-a941-472a05b53921"
      },
      "source": [
        "train_set = Bicicletinha('bike_train.csv')\n",
        "test_set  = Bicicletinha('bike_test.csv')\n",
        "\n",
        "print('Tamanho do treino: ' + str(len(train_set)) + ' amostras')\n",
        "print('Tamanho do teste: ' + str(len(test_set)) + ' amostras')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tamanho do treino: 13903 amostras\n",
            "Tamanho do teste: 3476 amostras\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZq6iuq6lQ9N",
        "colab_type": "text"
      },
      "source": [
        "## Dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuETOc64MynK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Criando dataloader\n",
        "train_loader = DataLoader(train_set,\n",
        "                          args['batch_size'],\n",
        "                          num_workers=args['num_workers'],\n",
        "                          shuffle=True)\n",
        "test_loader = DataLoader(test_set,\n",
        "                         args['batch_size'],\n",
        "                         num_workers=args['num_workers'],\n",
        "                         shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_wBx0Uesrgu",
        "colab_type": "text"
      },
      "source": [
        "O objeto retornado é um **iterador**, podendo ser utilizado para iterar em loops mas não suportando indexação."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zw1aAcsVCyp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9f65eed9-0aec-4849-fd61-6f1c5f9b5f55"
      },
      "source": [
        "for batch in test_loader:\n",
        "  \n",
        "  dado, rotulo = batch\n",
        "  print('## Dimensionalidade do batch ##')\n",
        "  print(dado.size(), rotulo.size())\n",
        "  \n",
        "  break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "## Dimensionalidade do batch ##\n",
            "torch.Size([20, 12]) torch.Size([20, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPCE1MC8yf02",
        "colab_type": "text"
      },
      "source": [
        "## Implementando o MLP\n",
        "\n",
        "Essa parte aqui você já tira de letra! Minha sugestão é construir um modelo com:\n",
        "\n",
        "* **Duas camadas escondidas**. Lembre-se de alternar as camadas com ativações não-lineares. \n",
        "* Uma camada de saída (com qual ativação?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-9spUHCUzBm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "ff71e483-f77b-4086-feac-a60be74625a5"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "  \n",
        "  def __init__(self, input_size, hidden_size, out_size):\n",
        "    super(MLP, self).__init__()\n",
        "    \n",
        "    self.features = nn.Sequential(\n",
        "          nn.Linear(input_size, hidden_size),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(hidden_size, hidden_size),\n",
        "          nn.ReLU(),\n",
        "    )\n",
        "    \n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Linear(hidden_size, out_size),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "\n",
        "  def forward(self, X):\n",
        "    \n",
        "    hidden = self.features(X)\n",
        "    output = self.classifier(hidden)\n",
        "    \n",
        "    return output\n",
        "\n",
        "input_size  = train_set[0][0].size(0)\n",
        "hidden_size = 128\n",
        "out_size    = 1\n",
        "\n",
        "net = MLP(input_size, hidden_size, out_size).to(args['device'])\n",
        "print(net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLP(\n",
            "  (features): Sequential(\n",
            "    (0): Linear(in_features=12, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=128, out_features=1, bias=True)\n",
            "    (1): ReLU()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb7zTsWV1cyQ",
        "colab_type": "text"
      },
      "source": [
        "## Definindo loss e otimizador\n",
        "\n",
        "Se lembra quais as funções de perda adequadas para um problema de regressão?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nx4MecnX1e2E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.L1Loss().to(args['device'])\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ujnT7rl0bjg",
        "colab_type": "text"
      },
      "source": [
        "# Fluxo de Treinamento & Validação\n",
        "\n",
        "## Treinamento\n",
        "\n",
        "Relembrando o passo a passo do fluxo de treinamento:\n",
        "* Iterar nas épocas\n",
        "* Iterar nos batches\n",
        "* Cast dos dados no dispositivo de hardware\n",
        "* Forward na rede e cálculo da loss\n",
        "* Cálculo do gradiente e atualização dos pesos\n",
        "\n",
        "Esse conjunto de passos é responsável pelo processo iterativo de otimização de uma rede. **A validação** por outro lado, é apenas a aplicação da rede em dados nunca antes visto para estimar a qualidade do modelo no mundo real.\n",
        "\n",
        "## Validação\n",
        "\n",
        "Para essa etapa, o PyTorch oferece dois artifícios:\n",
        "* ```model.eval()```: Impacta no *forward* da rede, informando as camadas caso seu comportamento mude entre fluxos (ex: dropout).\n",
        "* ```with torch.no_grad()```: Gerenciador de contexto que desabilita o cálculo e armazenamento de gradientes (economia de tempo e memória). Todo o código de validação deve ser executado dentro desse contexto.\n",
        "\n",
        "Exemplo de código para validação\n",
        "\n",
        "```python\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "  for batch in test_loader:\n",
        "      # Código de validação\n",
        "```\n",
        "\n",
        "Existe o equivalente ao ```model.eval()``` para explicitar que a sua rede deve estar em modo de treino, é o ```model.train()```. Apesar de ser o padrão dos modelos, é boa prática definir também o modo de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUwXgLyM4V2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_loader, net, epoch):\n",
        "\n",
        "  # Training mode\n",
        "  net.train()\n",
        "  \n",
        "  start = time.time()\n",
        "  \n",
        "  epoch_loss  = []\n",
        "  for batch in train_loader:\n",
        "    \n",
        "    dado, rotulo = batch\n",
        "    \n",
        "    # Cast do dado na GPU\n",
        "    dado = dado.to(args['device'])\n",
        "    rotulo = rotulo.to(args['device'])\n",
        "    \n",
        "    # Forward\n",
        "    ypred = net(dado)\n",
        "    loss = criterion(ypred, rotulo)\n",
        "    epoch_loss.append(loss.cpu().data)\n",
        "    \n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "   \n",
        "  epoch_loss = np.asarray(epoch_loss)\n",
        "  \n",
        "  end = time.time()\n",
        "  print('#################### Train ####################')\n",
        "  print('Epoch %d, Loss: %.4f +/- %.4f, Time: %.2f' % (epoch, epoch_loss.mean(), epoch_loss.std(), end-start))\n",
        "  \n",
        "  return epoch_loss.mean()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79KDmwyL4l89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(test_loader, net, epoch):\n",
        "\n",
        "  # Evaluation mode\n",
        "  net.eval()\n",
        "  \n",
        "  start = time.time()\n",
        "  \n",
        "  epoch_loss  = []\n",
        "  \n",
        "  with torch.no_grad(): \n",
        "    for batch in test_loader:\n",
        "\n",
        "      dado, rotulo = batch\n",
        "\n",
        "      # Cast do dado na GPU\n",
        "      dado = dado.to(args['device'])\n",
        "      rotulo = rotulo.to(args['device'])\n",
        "\n",
        "      # Forward\n",
        "      ypred = net(dado)\n",
        "      loss = criterion(ypred, rotulo)\n",
        "      epoch_loss.append(loss.cpu().data)\n",
        "\n",
        "  epoch_loss = np.asarray(epoch_loss)\n",
        "  \n",
        "  end = time.time()\n",
        "  print('********** Validate **********')\n",
        "  print('Epoch %d, Loss: %.4f +/- %.4f, Time: %.2f\\n' % (epoch, epoch_loss.mean(), epoch_loss.std(), end-start))\n",
        "  \n",
        "  return epoch_loss.mean()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95dKe7by6Qot",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e48a5af3-413a-4e41-bf06-5abc79915450"
      },
      "source": [
        "train_losses, test_losses = [], []\n",
        "for epoch in range(args['epoch_num']):\n",
        "  \n",
        "  # Train\n",
        "  train_losses.append(train(train_loader, net, epoch))\n",
        "  \n",
        "  # Validate\n",
        "  test_losses.append(validate(test_loader, net, epoch))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#################### Train ####################\n",
            "Epoch 0, Loss: 164.4053 +/- 40.4559, Time: 3.03\n",
            "********** Validate **********\n",
            "Epoch 0, Loss: 126.4238 +/- 30.5644, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 1, Loss: 131.9690 +/- 24.8562, Time: 2.89\n",
            "********** Validate **********\n",
            "Epoch 1, Loss: 127.6090 +/- 24.2036, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 2, Loss: 124.1441 +/- 30.5720, Time: 2.80\n",
            "********** Validate **********\n",
            "Epoch 2, Loss: 125.6277 +/- 31.5972, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 3, Loss: 119.9048 +/- 28.3566, Time: 2.82\n",
            "********** Validate **********\n",
            "Epoch 3, Loss: 118.9748 +/- 25.8841, Time: 0.52\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 4, Loss: 118.1719 +/- 25.3235, Time: 2.87\n",
            "********** Validate **********\n",
            "Epoch 4, Loss: 115.5986 +/- 29.4684, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 5, Loss: 116.1289 +/- 30.5013, Time: 2.88\n",
            "********** Validate **********\n",
            "Epoch 5, Loss: 113.7074 +/- 27.2139, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 6, Loss: 113.9078 +/- 27.0090, Time: 2.86\n",
            "********** Validate **********\n",
            "Epoch 6, Loss: 111.7463 +/- 28.9512, Time: 0.51\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 7, Loss: 110.9006 +/- 28.6426, Time: 2.83\n",
            "********** Validate **********\n",
            "Epoch 7, Loss: 109.9580 +/- 25.7642, Time: 0.56\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 8, Loss: 107.1822 +/- 26.3983, Time: 2.92\n",
            "********** Validate **********\n",
            "Epoch 8, Loss: 103.5158 +/- 25.9427, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 9, Loss: 102.3257 +/- 26.6194, Time: 2.87\n",
            "********** Validate **********\n",
            "Epoch 9, Loss: 98.2190 +/- 25.9957, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 10, Loss: 97.6226 +/- 25.4929, Time: 2.86\n",
            "********** Validate **********\n",
            "Epoch 10, Loss: 93.6248 +/- 24.9119, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 11, Loss: 95.4377 +/- 24.0500, Time: 2.87\n",
            "********** Validate **********\n",
            "Epoch 11, Loss: 91.8685 +/- 23.4518, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 12, Loss: 93.9781 +/- 23.2913, Time: 2.88\n",
            "********** Validate **********\n",
            "Epoch 12, Loss: 92.2886 +/- 22.0982, Time: 0.52\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 13, Loss: 91.7978 +/- 23.5764, Time: 2.91\n",
            "********** Validate **********\n",
            "Epoch 13, Loss: 87.5439 +/- 22.5809, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 14, Loss: 90.1043 +/- 24.5882, Time: 2.90\n",
            "********** Validate **********\n",
            "Epoch 14, Loss: 88.0151 +/- 23.5016, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 15, Loss: 89.0326 +/- 23.3670, Time: 2.91\n",
            "********** Validate **********\n",
            "Epoch 15, Loss: 87.0118 +/- 21.8088, Time: 0.52\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 16, Loss: 88.4617 +/- 23.0953, Time: 2.81\n",
            "********** Validate **********\n",
            "Epoch 16, Loss: 86.7269 +/- 22.9960, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 17, Loss: 90.9867 +/- 24.1189, Time: 2.90\n",
            "********** Validate **********\n",
            "Epoch 17, Loss: 94.4146 +/- 25.5334, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 18, Loss: 90.6920 +/- 23.9080, Time: 2.82\n",
            "********** Validate **********\n",
            "Epoch 18, Loss: 90.5026 +/- 25.1757, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 19, Loss: 88.8444 +/- 24.4490, Time: 2.85\n",
            "********** Validate **********\n",
            "Epoch 19, Loss: 91.0391 +/- 25.6552, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 20, Loss: 88.8650 +/- 22.9952, Time: 2.93\n",
            "********** Validate **********\n",
            "Epoch 20, Loss: 90.9129 +/- 25.8130, Time: 0.56\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 21, Loss: 88.6890 +/- 22.9456, Time: 2.84\n",
            "********** Validate **********\n",
            "Epoch 21, Loss: 84.7220 +/- 24.0940, Time: 0.52\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 22, Loss: 88.7891 +/- 23.0429, Time: 2.92\n",
            "********** Validate **********\n",
            "Epoch 22, Loss: 84.7608 +/- 21.6624, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 23, Loss: 87.7842 +/- 26.0931, Time: 2.84\n",
            "********** Validate **********\n",
            "Epoch 23, Loss: 91.7796 +/- 21.3581, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 24, Loss: 86.8404 +/- 25.1938, Time: 2.81\n",
            "********** Validate **********\n",
            "Epoch 24, Loss: 81.5518 +/- 21.8862, Time: 0.51\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 25, Loss: 86.7307 +/- 23.7597, Time: 2.83\n",
            "********** Validate **********\n",
            "Epoch 25, Loss: 88.6595 +/- 25.6817, Time: 0.52\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 26, Loss: 85.2184 +/- 21.6467, Time: 2.83\n",
            "********** Validate **********\n",
            "Epoch 26, Loss: 79.6904 +/- 22.5679, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 27, Loss: 85.7174 +/- 23.9874, Time: 2.81\n",
            "********** Validate **********\n",
            "Epoch 27, Loss: 87.6871 +/- 20.6657, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 28, Loss: 84.7299 +/- 22.9083, Time: 2.85\n",
            "********** Validate **********\n",
            "Epoch 28, Loss: 80.7178 +/- 23.4056, Time: 0.52\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 29, Loss: 83.0481 +/- 21.2156, Time: 2.91\n",
            "********** Validate **********\n",
            "Epoch 29, Loss: 82.8217 +/- 24.4817, Time: 0.52\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 30, Loss: 85.0404 +/- 23.6174, Time: 2.89\n",
            "********** Validate **********\n",
            "Epoch 30, Loss: 84.8080 +/- 20.6011, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 31, Loss: 82.2244 +/- 23.7771, Time: 2.87\n",
            "********** Validate **********\n",
            "Epoch 31, Loss: 78.3982 +/- 21.3806, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 32, Loss: 81.7467 +/- 22.3440, Time: 2.86\n",
            "********** Validate **********\n",
            "Epoch 32, Loss: 83.4031 +/- 24.4630, Time: 0.50\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 33, Loss: 81.7529 +/- 22.6582, Time: 2.90\n",
            "********** Validate **********\n",
            "Epoch 33, Loss: 78.3504 +/- 20.6646, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 34, Loss: 79.6094 +/- 23.2759, Time: 2.79\n",
            "********** Validate **********\n",
            "Epoch 34, Loss: 78.8845 +/- 20.0186, Time: 0.51\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 35, Loss: 80.9256 +/- 22.6373, Time: 2.92\n",
            "********** Validate **********\n",
            "Epoch 35, Loss: 81.2103 +/- 24.0820, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 36, Loss: 79.2415 +/- 21.3017, Time: 2.95\n",
            "********** Validate **********\n",
            "Epoch 36, Loss: 75.1927 +/- 20.7779, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 37, Loss: 78.2342 +/- 22.1682, Time: 2.86\n",
            "********** Validate **********\n",
            "Epoch 37, Loss: 78.2555 +/- 19.6644, Time: 0.52\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 38, Loss: 77.2991 +/- 21.0349, Time: 2.88\n",
            "********** Validate **********\n",
            "Epoch 38, Loss: 74.1858 +/- 21.5752, Time: 0.52\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 39, Loss: 75.9240 +/- 22.4266, Time: 2.89\n",
            "********** Validate **********\n",
            "Epoch 39, Loss: 74.7173 +/- 21.6105, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 40, Loss: 74.4881 +/- 20.8736, Time: 2.77\n",
            "********** Validate **********\n",
            "Epoch 40, Loss: 72.0039 +/- 21.2594, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 41, Loss: 73.3696 +/- 21.7849, Time: 2.92\n",
            "********** Validate **********\n",
            "Epoch 41, Loss: 71.8108 +/- 19.7829, Time: 0.52\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 42, Loss: 72.4134 +/- 20.7573, Time: 2.89\n",
            "********** Validate **********\n",
            "Epoch 42, Loss: 70.5949 +/- 20.5288, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 43, Loss: 71.6124 +/- 20.1392, Time: 2.84\n",
            "********** Validate **********\n",
            "Epoch 43, Loss: 69.3748 +/- 20.0076, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 44, Loss: 71.1191 +/- 19.1233, Time: 2.94\n",
            "********** Validate **********\n",
            "Epoch 44, Loss: 69.8247 +/- 19.6565, Time: 0.52\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 45, Loss: 69.7454 +/- 19.7704, Time: 2.95\n",
            "********** Validate **********\n",
            "Epoch 45, Loss: 67.0824 +/- 19.6590, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 46, Loss: 69.0448 +/- 19.6216, Time: 2.89\n",
            "********** Validate **********\n",
            "Epoch 46, Loss: 67.0555 +/- 19.5048, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 47, Loss: 68.7136 +/- 19.8552, Time: 2.96\n",
            "********** Validate **********\n",
            "Epoch 47, Loss: 66.4737 +/- 19.5606, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 48, Loss: 68.2943 +/- 19.9869, Time: 2.86\n",
            "********** Validate **********\n",
            "Epoch 48, Loss: 65.6362 +/- 19.7180, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 49, Loss: 68.2105 +/- 19.5521, Time: 2.93\n",
            "********** Validate **********\n",
            "Epoch 49, Loss: 65.0363 +/- 19.1934, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 50, Loss: 68.6042 +/- 19.9910, Time: 2.84\n",
            "********** Validate **********\n",
            "Epoch 50, Loss: 68.2952 +/- 18.9697, Time: 0.52\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 51, Loss: 69.1400 +/- 20.7695, Time: 2.86\n",
            "********** Validate **********\n",
            "Epoch 51, Loss: 67.0149 +/- 20.3665, Time: 0.59\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 52, Loss: 70.3451 +/- 19.2818, Time: 2.90\n",
            "********** Validate **********\n",
            "Epoch 52, Loss: 63.9262 +/- 19.2959, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 53, Loss: 69.9053 +/- 18.3761, Time: 2.90\n",
            "********** Validate **********\n",
            "Epoch 53, Loss: 67.6715 +/- 18.2695, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 54, Loss: 70.6174 +/- 19.3680, Time: 2.91\n",
            "********** Validate **********\n",
            "Epoch 54, Loss: 65.4102 +/- 19.0740, Time: 0.56\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 55, Loss: 69.1616 +/- 19.7414, Time: 2.91\n",
            "********** Validate **********\n",
            "Epoch 55, Loss: 69.9086 +/- 18.5057, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 56, Loss: 69.4349 +/- 20.0711, Time: 2.92\n",
            "********** Validate **********\n",
            "Epoch 56, Loss: 69.3046 +/- 18.7647, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 57, Loss: 68.5367 +/- 18.9361, Time: 2.93\n",
            "********** Validate **********\n",
            "Epoch 57, Loss: 68.1478 +/- 20.6879, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 58, Loss: 67.0561 +/- 17.4122, Time: 2.88\n",
            "********** Validate **********\n",
            "Epoch 58, Loss: 65.0161 +/- 18.0644, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 59, Loss: 65.7205 +/- 19.4853, Time: 2.93\n",
            "********** Validate **********\n",
            "Epoch 59, Loss: 62.7316 +/- 18.0397, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 60, Loss: 65.4217 +/- 17.9973, Time: 2.91\n",
            "********** Validate **********\n",
            "Epoch 60, Loss: 65.3205 +/- 19.5367, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 61, Loss: 65.4565 +/- 19.6651, Time: 3.03\n",
            "********** Validate **********\n",
            "Epoch 61, Loss: 64.1693 +/- 17.5656, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 62, Loss: 64.1839 +/- 18.2815, Time: 3.01\n",
            "********** Validate **********\n",
            "Epoch 62, Loss: 61.0703 +/- 17.3245, Time: 0.52\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 63, Loss: 63.5815 +/- 18.0733, Time: 2.94\n",
            "********** Validate **********\n",
            "Epoch 63, Loss: 62.5988 +/- 18.8547, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 64, Loss: 62.7948 +/- 16.5049, Time: 2.85\n",
            "********** Validate **********\n",
            "Epoch 64, Loss: 60.8932 +/- 17.9510, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 65, Loss: 62.8505 +/- 17.7594, Time: 2.93\n",
            "********** Validate **********\n",
            "Epoch 65, Loss: 63.5230 +/- 17.4904, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 66, Loss: 62.3129 +/- 17.2471, Time: 2.87\n",
            "********** Validate **********\n",
            "Epoch 66, Loss: 60.9020 +/- 17.6189, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 67, Loss: 62.0430 +/- 17.3171, Time: 2.96\n",
            "********** Validate **********\n",
            "Epoch 67, Loss: 61.3535 +/- 18.0329, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 68, Loss: 60.8741 +/- 17.2550, Time: 2.90\n",
            "********** Validate **********\n",
            "Epoch 68, Loss: 59.5425 +/- 18.0294, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 69, Loss: 60.8407 +/- 16.6599, Time: 2.94\n",
            "********** Validate **********\n",
            "Epoch 69, Loss: 59.2762 +/- 17.2796, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 70, Loss: 60.9201 +/- 17.8906, Time: 2.89\n",
            "********** Validate **********\n",
            "Epoch 70, Loss: 61.9734 +/- 17.4907, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 71, Loss: 60.8835 +/- 16.6415, Time: 2.89\n",
            "********** Validate **********\n",
            "Epoch 71, Loss: 59.6911 +/- 17.1399, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 72, Loss: 61.5406 +/- 16.1876, Time: 2.89\n",
            "********** Validate **********\n",
            "Epoch 72, Loss: 61.1765 +/- 18.3161, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 73, Loss: 60.8679 +/- 16.5779, Time: 2.91\n",
            "********** Validate **********\n",
            "Epoch 73, Loss: 59.3677 +/- 17.2195, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 74, Loss: 60.2446 +/- 16.8152, Time: 2.91\n",
            "********** Validate **********\n",
            "Epoch 74, Loss: 61.9246 +/- 16.9717, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 75, Loss: 59.7574 +/- 17.1887, Time: 2.92\n",
            "********** Validate **********\n",
            "Epoch 75, Loss: 58.6431 +/- 17.2947, Time: 0.56\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 76, Loss: 59.4027 +/- 15.6714, Time: 2.88\n",
            "********** Validate **********\n",
            "Epoch 76, Loss: 60.4795 +/- 17.7331, Time: 0.52\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 77, Loss: 59.4258 +/- 16.3560, Time: 2.86\n",
            "********** Validate **********\n",
            "Epoch 77, Loss: 58.6467 +/- 16.9593, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 78, Loss: 59.0323 +/- 17.2840, Time: 2.84\n",
            "********** Validate **********\n",
            "Epoch 78, Loss: 58.5349 +/- 16.5426, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 79, Loss: 59.3029 +/- 16.2314, Time: 2.86\n",
            "********** Validate **********\n",
            "Epoch 79, Loss: 58.4457 +/- 16.7913, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 80, Loss: 59.5550 +/- 16.0174, Time: 2.93\n",
            "********** Validate **********\n",
            "Epoch 80, Loss: 57.5607 +/- 16.7135, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 81, Loss: 59.3772 +/- 16.4972, Time: 2.93\n",
            "********** Validate **********\n",
            "Epoch 81, Loss: 58.6992 +/- 16.5292, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 82, Loss: 58.9889 +/- 15.2481, Time: 2.93\n",
            "********** Validate **********\n",
            "Epoch 82, Loss: 58.0962 +/- 16.8375, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 83, Loss: 58.6233 +/- 15.7874, Time: 2.93\n",
            "********** Validate **********\n",
            "Epoch 83, Loss: 58.6186 +/- 16.9541, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 84, Loss: 57.9930 +/- 15.0984, Time: 2.87\n",
            "********** Validate **********\n",
            "Epoch 84, Loss: 59.7814 +/- 16.2538, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 85, Loss: 57.7842 +/- 15.6693, Time: 3.02\n",
            "********** Validate **********\n",
            "Epoch 85, Loss: 59.4961 +/- 16.9017, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 86, Loss: 57.6619 +/- 16.1170, Time: 3.05\n",
            "********** Validate **********\n",
            "Epoch 86, Loss: 59.6383 +/- 17.1212, Time: 0.56\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 87, Loss: 57.4216 +/- 16.1989, Time: 3.06\n",
            "********** Validate **********\n",
            "Epoch 87, Loss: 56.6774 +/- 16.1710, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 88, Loss: 56.8000 +/- 15.7641, Time: 2.86\n",
            "********** Validate **********\n",
            "Epoch 88, Loss: 58.5372 +/- 16.0442, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 89, Loss: 56.4352 +/- 15.9318, Time: 2.91\n",
            "********** Validate **********\n",
            "Epoch 89, Loss: 56.7500 +/- 16.0594, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 90, Loss: 55.5992 +/- 15.3551, Time: 2.91\n",
            "********** Validate **********\n",
            "Epoch 90, Loss: 56.1373 +/- 16.4092, Time: 0.52\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 91, Loss: 55.5326 +/- 15.0764, Time: 2.85\n",
            "********** Validate **********\n",
            "Epoch 91, Loss: 56.3379 +/- 16.2472, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 92, Loss: 55.0826 +/- 16.0810, Time: 2.91\n",
            "********** Validate **********\n",
            "Epoch 92, Loss: 56.4605 +/- 16.1293, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 93, Loss: 55.1207 +/- 15.1641, Time: 2.89\n",
            "********** Validate **********\n",
            "Epoch 93, Loss: 56.8928 +/- 16.3139, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 94, Loss: 55.1153 +/- 15.0025, Time: 2.96\n",
            "********** Validate **********\n",
            "Epoch 94, Loss: 55.6494 +/- 15.5612, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 95, Loss: 54.7969 +/- 15.6851, Time: 2.95\n",
            "********** Validate **********\n",
            "Epoch 95, Loss: 57.2610 +/- 15.7140, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 96, Loss: 54.6987 +/- 15.3158, Time: 2.93\n",
            "********** Validate **********\n",
            "Epoch 96, Loss: 55.3573 +/- 15.6349, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 97, Loss: 54.1842 +/- 15.4344, Time: 2.89\n",
            "********** Validate **********\n",
            "Epoch 97, Loss: 54.7710 +/- 15.9914, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 98, Loss: 53.5592 +/- 15.3719, Time: 2.87\n",
            "********** Validate **********\n",
            "Epoch 98, Loss: 56.1718 +/- 14.9998, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 99, Loss: 53.8511 +/- 15.1447, Time: 2.94\n",
            "********** Validate **********\n",
            "Epoch 99, Loss: 55.7746 +/- 14.9497, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 100, Loss: 53.7137 +/- 14.7111, Time: 2.94\n",
            "********** Validate **********\n",
            "Epoch 100, Loss: 54.8340 +/- 15.1977, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 101, Loss: 53.6383 +/- 14.5227, Time: 2.93\n",
            "********** Validate **********\n",
            "Epoch 101, Loss: 56.0381 +/- 16.0689, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 102, Loss: 53.4851 +/- 15.1297, Time: 2.94\n",
            "********** Validate **********\n",
            "Epoch 102, Loss: 54.9232 +/- 14.9632, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 103, Loss: 53.1664 +/- 14.3878, Time: 2.90\n",
            "********** Validate **********\n",
            "Epoch 103, Loss: 55.7781 +/- 14.9285, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 104, Loss: 53.0318 +/- 15.1459, Time: 2.95\n",
            "********** Validate **********\n",
            "Epoch 104, Loss: 53.4249 +/- 15.2354, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 105, Loss: 53.3157 +/- 14.0394, Time: 2.92\n",
            "********** Validate **********\n",
            "Epoch 105, Loss: 55.0786 +/- 15.3374, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 106, Loss: 53.4946 +/- 14.7190, Time: 2.91\n",
            "********** Validate **********\n",
            "Epoch 106, Loss: 54.2847 +/- 14.7722, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 107, Loss: 52.6355 +/- 14.2185, Time: 2.97\n",
            "********** Validate **********\n",
            "Epoch 107, Loss: 54.9481 +/- 14.7191, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 108, Loss: 53.5198 +/- 14.5628, Time: 2.90\n",
            "********** Validate **********\n",
            "Epoch 108, Loss: 54.3773 +/- 15.4841, Time: 0.56\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 109, Loss: 53.1851 +/- 14.6504, Time: 2.98\n",
            "********** Validate **********\n",
            "Epoch 109, Loss: 53.9929 +/- 14.8561, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 110, Loss: 52.7942 +/- 15.3431, Time: 2.97\n",
            "********** Validate **********\n",
            "Epoch 110, Loss: 54.7930 +/- 14.5833, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 111, Loss: 52.8499 +/- 13.9204, Time: 2.89\n",
            "********** Validate **********\n",
            "Epoch 111, Loss: 53.6880 +/- 14.9493, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 112, Loss: 52.4964 +/- 14.3819, Time: 2.91\n",
            "********** Validate **********\n",
            "Epoch 112, Loss: 54.3424 +/- 14.7913, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 113, Loss: 51.5058 +/- 13.7745, Time: 2.86\n",
            "********** Validate **********\n",
            "Epoch 113, Loss: 52.1866 +/- 14.2701, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 114, Loss: 50.3936 +/- 13.9065, Time: 2.90\n",
            "********** Validate **********\n",
            "Epoch 114, Loss: 51.5399 +/- 14.3483, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 115, Loss: 50.6406 +/- 13.7068, Time: 2.93\n",
            "********** Validate **********\n",
            "Epoch 115, Loss: 53.2944 +/- 14.3299, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 116, Loss: 50.6129 +/- 13.6454, Time: 2.93\n",
            "********** Validate **********\n",
            "Epoch 116, Loss: 51.3208 +/- 14.4026, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 117, Loss: 49.7774 +/- 14.0215, Time: 2.93\n",
            "********** Validate **********\n",
            "Epoch 117, Loss: 51.1099 +/- 14.4691, Time: 0.56\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 118, Loss: 49.4722 +/- 13.8475, Time: 2.92\n",
            "********** Validate **********\n",
            "Epoch 118, Loss: 51.8366 +/- 14.0559, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 119, Loss: 49.2906 +/- 13.3827, Time: 2.92\n",
            "********** Validate **********\n",
            "Epoch 119, Loss: 52.3101 +/- 14.0146, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 120, Loss: 49.2479 +/- 14.4498, Time: 2.99\n",
            "********** Validate **********\n",
            "Epoch 120, Loss: 51.0806 +/- 13.9658, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 121, Loss: 49.0978 +/- 13.8111, Time: 2.91\n",
            "********** Validate **********\n",
            "Epoch 121, Loss: 51.3856 +/- 14.2125, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 122, Loss: 49.3551 +/- 13.1786, Time: 2.86\n",
            "********** Validate **********\n",
            "Epoch 122, Loss: 51.4740 +/- 14.4234, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 123, Loss: 49.2918 +/- 13.8740, Time: 2.90\n",
            "********** Validate **********\n",
            "Epoch 123, Loss: 51.9338 +/- 13.9555, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 124, Loss: 48.9261 +/- 13.6735, Time: 2.95\n",
            "********** Validate **********\n",
            "Epoch 124, Loss: 50.7317 +/- 14.2491, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 125, Loss: 48.5073 +/- 13.1226, Time: 2.91\n",
            "********** Validate **********\n",
            "Epoch 125, Loss: 51.1679 +/- 14.2199, Time: 0.56\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 126, Loss: 47.8854 +/- 13.8656, Time: 2.91\n",
            "********** Validate **********\n",
            "Epoch 126, Loss: 49.7863 +/- 13.7161, Time: 0.52\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 127, Loss: 47.6835 +/- 13.5125, Time: 2.82\n",
            "********** Validate **********\n",
            "Epoch 127, Loss: 49.8632 +/- 13.7744, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 128, Loss: 47.9587 +/- 13.4578, Time: 2.85\n",
            "********** Validate **********\n",
            "Epoch 128, Loss: 51.2270 +/- 14.0504, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 129, Loss: 47.4356 +/- 12.9684, Time: 2.82\n",
            "********** Validate **********\n",
            "Epoch 129, Loss: 49.3351 +/- 13.8578, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 130, Loss: 47.6774 +/- 13.1663, Time: 2.92\n",
            "********** Validate **********\n",
            "Epoch 130, Loss: 50.4102 +/- 14.5957, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 131, Loss: 47.3520 +/- 12.6341, Time: 2.91\n",
            "********** Validate **********\n",
            "Epoch 131, Loss: 49.9722 +/- 13.7829, Time: 0.52\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 132, Loss: 47.1196 +/- 12.8476, Time: 2.79\n",
            "********** Validate **********\n",
            "Epoch 132, Loss: 50.3400 +/- 13.9418, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 133, Loss: 46.9397 +/- 12.6985, Time: 2.88\n",
            "********** Validate **********\n",
            "Epoch 133, Loss: 49.2118 +/- 13.7981, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 134, Loss: 46.9583 +/- 12.4967, Time: 2.88\n",
            "********** Validate **********\n",
            "Epoch 134, Loss: 50.2845 +/- 14.1237, Time: 0.51\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 135, Loss: 47.0235 +/- 13.4136, Time: 2.86\n",
            "********** Validate **********\n",
            "Epoch 135, Loss: 50.1132 +/- 13.8774, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 136, Loss: 46.5012 +/- 12.6695, Time: 2.85\n",
            "********** Validate **********\n",
            "Epoch 136, Loss: 48.1165 +/- 13.8782, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 137, Loss: 46.8268 +/- 13.3992, Time: 2.78\n",
            "********** Validate **********\n",
            "Epoch 137, Loss: 48.9235 +/- 13.4609, Time: 0.51\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 138, Loss: 47.4838 +/- 13.1204, Time: 2.83\n",
            "********** Validate **********\n",
            "Epoch 138, Loss: 49.8520 +/- 13.9909, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 139, Loss: 47.4931 +/- 13.3768, Time: 2.85\n",
            "********** Validate **********\n",
            "Epoch 139, Loss: 49.1338 +/- 13.8683, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 140, Loss: 47.3520 +/- 12.7390, Time: 2.88\n",
            "********** Validate **********\n",
            "Epoch 140, Loss: 48.1437 +/- 13.8587, Time: 0.51\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 141, Loss: 47.1886 +/- 13.2971, Time: 2.87\n",
            "********** Validate **********\n",
            "Epoch 141, Loss: 49.8868 +/- 13.7395, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 142, Loss: 47.5424 +/- 12.9725, Time: 2.84\n",
            "********** Validate **********\n",
            "Epoch 142, Loss: 51.6204 +/- 14.1194, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 143, Loss: 47.1449 +/- 12.3353, Time: 2.96\n",
            "********** Validate **********\n",
            "Epoch 143, Loss: 50.9253 +/- 13.9795, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 144, Loss: 47.1478 +/- 12.8653, Time: 2.89\n",
            "********** Validate **********\n",
            "Epoch 144, Loss: 50.9721 +/- 13.5257, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 145, Loss: 46.9483 +/- 12.4566, Time: 2.90\n",
            "********** Validate **********\n",
            "Epoch 145, Loss: 48.5189 +/- 13.3498, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 146, Loss: 46.7117 +/- 12.2924, Time: 2.95\n",
            "********** Validate **********\n",
            "Epoch 146, Loss: 48.2222 +/- 12.7548, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 147, Loss: 46.3507 +/- 12.7220, Time: 2.95\n",
            "********** Validate **********\n",
            "Epoch 147, Loss: 50.1698 +/- 14.2826, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 148, Loss: 45.9832 +/- 12.7118, Time: 2.96\n",
            "********** Validate **********\n",
            "Epoch 148, Loss: 48.4562 +/- 12.8356, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 149, Loss: 45.6570 +/- 12.5030, Time: 2.96\n",
            "********** Validate **********\n",
            "Epoch 149, Loss: 48.8316 +/- 13.3613, Time: 0.56\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 150, Loss: 45.4333 +/- 12.0582, Time: 3.03\n",
            "********** Validate **********\n",
            "Epoch 150, Loss: 48.3819 +/- 13.1732, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 151, Loss: 44.8266 +/- 11.7296, Time: 2.99\n",
            "********** Validate **********\n",
            "Epoch 151, Loss: 47.4962 +/- 12.7008, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 152, Loss: 44.8682 +/- 12.1900, Time: 2.95\n",
            "********** Validate **********\n",
            "Epoch 152, Loss: 46.6796 +/- 12.7310, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 153, Loss: 44.2256 +/- 11.9768, Time: 2.94\n",
            "********** Validate **********\n",
            "Epoch 153, Loss: 47.4532 +/- 12.9965, Time: 0.56\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 154, Loss: 44.0981 +/- 12.1823, Time: 2.94\n",
            "********** Validate **********\n",
            "Epoch 154, Loss: 47.1173 +/- 13.0771, Time: 0.56\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 155, Loss: 43.4929 +/- 11.8765, Time: 2.98\n",
            "********** Validate **********\n",
            "Epoch 155, Loss: 45.6672 +/- 12.4211, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 156, Loss: 43.2607 +/- 11.6038, Time: 2.92\n",
            "********** Validate **********\n",
            "Epoch 156, Loss: 45.7310 +/- 12.4987, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 157, Loss: 43.1320 +/- 12.0855, Time: 2.96\n",
            "********** Validate **********\n",
            "Epoch 157, Loss: 46.3088 +/- 12.3755, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 158, Loss: 43.4397 +/- 12.0695, Time: 2.95\n",
            "********** Validate **********\n",
            "Epoch 158, Loss: 46.3296 +/- 12.7814, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 159, Loss: 43.9671 +/- 12.2368, Time: 2.91\n",
            "********** Validate **********\n",
            "Epoch 159, Loss: 45.4930 +/- 11.8698, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 160, Loss: 43.7218 +/- 12.6324, Time: 2.96\n",
            "********** Validate **********\n",
            "Epoch 160, Loss: 45.6719 +/- 12.6767, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 161, Loss: 43.7384 +/- 12.2432, Time: 2.95\n",
            "********** Validate **********\n",
            "Epoch 161, Loss: 46.1106 +/- 12.0106, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 162, Loss: 43.8195 +/- 11.8692, Time: 2.86\n",
            "********** Validate **********\n",
            "Epoch 162, Loss: 46.4769 +/- 12.5081, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 163, Loss: 43.9203 +/- 11.9813, Time: 2.94\n",
            "********** Validate **********\n",
            "Epoch 163, Loss: 47.7642 +/- 12.2742, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 164, Loss: 43.2356 +/- 11.7395, Time: 2.93\n",
            "********** Validate **********\n",
            "Epoch 164, Loss: 46.9473 +/- 12.0781, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 165, Loss: 43.9031 +/- 11.5723, Time: 2.91\n",
            "********** Validate **********\n",
            "Epoch 165, Loss: 45.6819 +/- 12.5062, Time: 0.52\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 166, Loss: 44.3286 +/- 11.8576, Time: 2.91\n",
            "********** Validate **********\n",
            "Epoch 166, Loss: 45.8140 +/- 11.9401, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 167, Loss: 44.0388 +/- 12.1048, Time: 2.96\n",
            "********** Validate **********\n",
            "Epoch 167, Loss: 47.9206 +/- 12.7595, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 168, Loss: 43.9637 +/- 11.9781, Time: 2.93\n",
            "********** Validate **********\n",
            "Epoch 168, Loss: 46.8355 +/- 12.2851, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 169, Loss: 44.1110 +/- 11.8124, Time: 2.96\n",
            "********** Validate **********\n",
            "Epoch 169, Loss: 45.8717 +/- 12.5034, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 170, Loss: 43.6859 +/- 12.0843, Time: 2.91\n",
            "********** Validate **********\n",
            "Epoch 170, Loss: 46.3266 +/- 11.9227, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 171, Loss: 43.2123 +/- 11.7374, Time: 2.88\n",
            "********** Validate **********\n",
            "Epoch 171, Loss: 45.3526 +/- 11.5452, Time: 0.52\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 172, Loss: 43.2291 +/- 12.4543, Time: 2.91\n",
            "********** Validate **********\n",
            "Epoch 172, Loss: 44.8243 +/- 12.2558, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 173, Loss: 43.3746 +/- 12.0753, Time: 2.93\n",
            "********** Validate **********\n",
            "Epoch 173, Loss: 45.4734 +/- 11.8071, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 174, Loss: 41.9941 +/- 11.7551, Time: 2.92\n",
            "********** Validate **********\n",
            "Epoch 174, Loss: 46.2333 +/- 12.2660, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 175, Loss: 42.2519 +/- 11.4686, Time: 3.00\n",
            "********** Validate **********\n",
            "Epoch 175, Loss: 44.5411 +/- 11.8125, Time: 0.56\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 176, Loss: 42.3595 +/- 11.4907, Time: 3.02\n",
            "********** Validate **********\n",
            "Epoch 176, Loss: 43.2738 +/- 11.3525, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 177, Loss: 41.8274 +/- 11.2683, Time: 3.01\n",
            "********** Validate **********\n",
            "Epoch 177, Loss: 45.1470 +/- 12.1496, Time: 0.56\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 178, Loss: 41.8496 +/- 11.5742, Time: 2.87\n",
            "********** Validate **********\n",
            "Epoch 178, Loss: 45.6510 +/- 11.5060, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 179, Loss: 41.7097 +/- 11.4981, Time: 2.92\n",
            "********** Validate **********\n",
            "Epoch 179, Loss: 44.3528 +/- 11.4896, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 180, Loss: 41.3230 +/- 11.6594, Time: 2.94\n",
            "********** Validate **********\n",
            "Epoch 180, Loss: 43.6914 +/- 11.9181, Time: 0.56\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 181, Loss: 41.1837 +/- 11.1842, Time: 2.88\n",
            "********** Validate **********\n",
            "Epoch 181, Loss: 43.4212 +/- 11.0896, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 182, Loss: 41.5560 +/- 11.3710, Time: 2.89\n",
            "********** Validate **********\n",
            "Epoch 182, Loss: 44.5752 +/- 11.5907, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 183, Loss: 41.5272 +/- 11.4239, Time: 2.89\n",
            "********** Validate **********\n",
            "Epoch 183, Loss: 43.7668 +/- 12.0698, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 184, Loss: 41.5986 +/- 11.6693, Time: 2.99\n",
            "********** Validate **********\n",
            "Epoch 184, Loss: 44.6481 +/- 11.4569, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 185, Loss: 41.0949 +/- 11.0042, Time: 2.92\n",
            "********** Validate **********\n",
            "Epoch 185, Loss: 45.6182 +/- 11.6867, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 186, Loss: 41.2973 +/- 11.1799, Time: 2.97\n",
            "********** Validate **********\n",
            "Epoch 186, Loss: 43.8810 +/- 11.8035, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 187, Loss: 40.8989 +/- 10.9586, Time: 2.94\n",
            "********** Validate **********\n",
            "Epoch 187, Loss: 43.8597 +/- 12.2060, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 188, Loss: 40.5895 +/- 10.7782, Time: 2.86\n",
            "********** Validate **********\n",
            "Epoch 188, Loss: 44.5164 +/- 11.1972, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 189, Loss: 39.9648 +/- 10.2674, Time: 2.93\n",
            "********** Validate **********\n",
            "Epoch 189, Loss: 42.6851 +/- 11.9214, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 190, Loss: 39.6534 +/- 10.8067, Time: 2.94\n",
            "********** Validate **********\n",
            "Epoch 190, Loss: 42.7896 +/- 11.3834, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 191, Loss: 39.8354 +/- 10.4397, Time: 2.98\n",
            "********** Validate **********\n",
            "Epoch 191, Loss: 41.7379 +/- 11.5215, Time: 0.56\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 192, Loss: 39.6159 +/- 11.2670, Time: 2.96\n",
            "********** Validate **********\n",
            "Epoch 192, Loss: 43.0932 +/- 11.0573, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 193, Loss: 39.8460 +/- 10.3398, Time: 2.92\n",
            "********** Validate **********\n",
            "Epoch 193, Loss: 42.6507 +/- 11.1330, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 194, Loss: 40.5127 +/- 10.7083, Time: 2.94\n",
            "********** Validate **********\n",
            "Epoch 194, Loss: 40.6413 +/- 11.2825, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 195, Loss: 39.7928 +/- 10.9139, Time: 2.94\n",
            "********** Validate **********\n",
            "Epoch 195, Loss: 40.8721 +/- 11.4010, Time: 0.54\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 196, Loss: 39.1596 +/- 10.8318, Time: 2.93\n",
            "********** Validate **********\n",
            "Epoch 196, Loss: 40.0863 +/- 11.2847, Time: 0.55\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 197, Loss: 38.9232 +/- 10.6704, Time: 2.92\n",
            "********** Validate **********\n",
            "Epoch 197, Loss: 40.9723 +/- 11.1814, Time: 0.56\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 198, Loss: 38.8161 +/- 10.5749, Time: 2.90\n",
            "********** Validate **********\n",
            "Epoch 198, Loss: 41.6350 +/- 11.2138, Time: 0.53\n",
            "\n",
            "#################### Train ####################\n",
            "Epoch 199, Loss: 38.7505 +/- 10.4427, Time: 2.94\n",
            "********** Validate **********\n",
            "Epoch 199, Loss: 41.0296 +/- 10.7014, Time: 0.54\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqTmpWzUV40g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "outputId": "4e0e2263-9acc-47b3-8e9f-aeaacb7d08a0"
      },
      "source": [
        "Xtest = torch.stack([tup[0] for tup in test_set])\n",
        "Xtest = Xtest.to(args['device'])\n",
        "\n",
        "ytest = torch.stack([tup[1] for tup in test_set])\n",
        "ypred = net(Xtest).cpu().data\n",
        "\n",
        "data = torch.cat((ytest, ypred), axis=1)\n",
        "\n",
        "df_results = pd.DataFrame(data, columns=['ypred', 'ytest'])\n",
        "df_results.head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ypred</th>\n",
              "      <th>ytest</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tensor(352.)</td>\n",
              "      <td>tensor(319.9706)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tensor(156.)</td>\n",
              "      <td>tensor(130.9763)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tensor(12.)</td>\n",
              "      <td>tensor(15.5401)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>tensor(2.)</td>\n",
              "      <td>tensor(0.)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tensor(391.)</td>\n",
              "      <td>tensor(356.3062)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>tensor(391.)</td>\n",
              "      <td>tensor(268.8271)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>tensor(84.)</td>\n",
              "      <td>tensor(93.7724)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>tensor(487.)</td>\n",
              "      <td>tensor(466.6277)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>tensor(176.)</td>\n",
              "      <td>tensor(342.1359)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>tensor(157.)</td>\n",
              "      <td>tensor(174.9841)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>tensor(82.)</td>\n",
              "      <td>tensor(97.3801)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>tensor(186.)</td>\n",
              "      <td>tensor(124.0032)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>tensor(277.)</td>\n",
              "      <td>tensor(258.3667)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>tensor(264.)</td>\n",
              "      <td>tensor(228.8760)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>tensor(312.)</td>\n",
              "      <td>tensor(292.6267)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>tensor(56.)</td>\n",
              "      <td>tensor(42.2758)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>tensor(370.)</td>\n",
              "      <td>tensor(284.2224)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>tensor(14.)</td>\n",
              "      <td>tensor(0.)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>tensor(124.)</td>\n",
              "      <td>tensor(147.3903)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>tensor(427.)</td>\n",
              "      <td>tensor(360.6158)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           ypred             ytest\n",
              "0   tensor(352.)  tensor(319.9706)\n",
              "1   tensor(156.)  tensor(130.9763)\n",
              "2    tensor(12.)   tensor(15.5401)\n",
              "3     tensor(2.)        tensor(0.)\n",
              "4   tensor(391.)  tensor(356.3062)\n",
              "5   tensor(391.)  tensor(268.8271)\n",
              "6    tensor(84.)   tensor(93.7724)\n",
              "7   tensor(487.)  tensor(466.6277)\n",
              "8   tensor(176.)  tensor(342.1359)\n",
              "9   tensor(157.)  tensor(174.9841)\n",
              "10   tensor(82.)   tensor(97.3801)\n",
              "11  tensor(186.)  tensor(124.0032)\n",
              "12  tensor(277.)  tensor(258.3667)\n",
              "13  tensor(264.)  tensor(228.8760)\n",
              "14  tensor(312.)  tensor(292.6267)\n",
              "15   tensor(56.)   tensor(42.2758)\n",
              "16  tensor(370.)  tensor(284.2224)\n",
              "17   tensor(14.)        tensor(0.)\n",
              "18  tensor(124.)  tensor(147.3903)\n",
              "19  tensor(427.)  tensor(360.6158)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I5L8uae15qz",
        "colab_type": "text"
      },
      "source": [
        "# Gráfico de convergência"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDM-Iki543ID",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "outputId": "c8787fdd-ff72-4aab-ec02-49bc13841f6c"
      },
      "source": [
        "plt.figure(figsize=(20, 9))\n",
        "plt.plot(train_losses, label='Train')\n",
        "plt.plot(test_losses, label='Test', linewidth=3, alpha=0.5)\n",
        "plt.xlabel('Epochs', fontsize=16)\n",
        "plt.ylabel('Loss', fontsize=16)\n",
        "plt.title('Convergence', fontsize=16)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABJoAAAItCAYAAABxdhBxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nERERERERkaT4/1zwgo2Z5saoAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1440x648 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}